---
layout: post
title: Shannon Entropy and Information Gain
tags:
- Machine Learning
- Machine learning
status: publish
type: post
published: true
meta: {}
---
Shannon Entropy and Information gain are some fundamental concepts used in classification algorithms like decision tree in machine learning. But before going deep into that let us first define <strong>what is information</strong>.Well, in a very crude sense information is the <em>number of bits</em> needed to represent a particular amount of data.
<h1>Information Gain</h1>
Let us say someone wants to crack a password. All he knows is that the password in five character long and contains letter from English alphabet only - there are no digit or special characters in that. He has $latex 5^{26}$ options to choose from. If he wants to put all those choices into a file or something, he would need some $latex \large{A}$ number of bits for that. Now lets say somehow he manages to find that the first two characters are 'g' and 'j'. The number of options for him now reduces to $latex 3^{26}$. Let us say he would need $latex \large{B}$ number of bits to represent that. The total number of bits needed by him now is $latex \large{B}$ plus some bits to represent 'g' and 'j'. Since the last value is very small we can ignore that for the most part.

Obviously $latex \large{B}$ is smaller than $latex \large{A}$. We define the difference $latex \large{A} - \large{B}$ as the information gain for this example. That is the number of bits he has saved by having that knowledge.

More formally we can define Information Gain as the following: Let $latex \bold{X}$ = $latex (X, p)$ be a probability space. The information gain $latex Gain(\bold{Y}|\bold{X})$ is the gain obtained by the knowledge that the outcome of an experiment belongs to a set $latex \bold{Y}$ where $latex \bold{Y} \subset \bold{X}$
<p style="text-align:center;">$latex Gain(\bold{Y}|\bold{X})$ = $latex \log_2{p(\bold{X})}$ - $latex \log_2{p(\bold{Y})}$ = $latex \log_2{\frac{p(\bold{X})}{p(\bold{Y})}}$ = $latex \log_2{\frac{1}{p(\bold{Y})}}$ = $latex - \log_2{p(\bold{Y})}$</p>

<h1 style="text-align:left;">Shannon Entropy</h1>
Entropy is defined as the messiness or disorderliness or data. The primary goal of any classification algorithm is to split the data into subsets so that the messiness in each individual group reduces and thus reducingÂ  the overall messiness. In other words the primary goal of any classification algorithm is to reduce the entropy of the data.

Shannon entropy is a mathematical model which can describe the disorderliness of a particular dataset. It is defined as
<p style="text-align:center;">$latex H(\bold{A})$ = $latex - \sum_{i=1}^{n} p_{i}\log_2{p_{i}}$</p>
